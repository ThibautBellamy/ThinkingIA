#!/usr/bin/env python3
"""
Chargeur de configuration YAML pour ThinkingIA
"""

import yaml
import torch
import os
from pathlib import Path
from typing import Dict, Any

class ConfigLoader:
    def __init__(self, config_path: str = "config.yaml"):
        self.config_path = Path(config_path)
        self.config_data = None
        
        # Charger la configuration
        self.load_config()
        
        # Post-traitement automatique
        self._post_process_config()
        
        # Validation
        self._validate_config()
    
    def load_config(self):
        """Charge le fichier YAML de configuration"""
        
        if not self.config_path.exists():
            print(f"âš ï¸  Fichier de config {self.config_path} non trouvÃ©")
            print("ğŸ“ CrÃ©ation d'un fichier de configuration par dÃ©faut...")
            self.create_default_config()
        
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config_data = yaml.safe_load(f)
            
            print(f"âœ… Configuration chargÃ©e depuis {self.config_path}")
            
        except yaml.YAMLError as e:
            raise ValueError(f"Erreur lors du parsing YAML: {e}")
        except Exception as e:
            raise ValueError(f"Erreur lors du chargement de la config: {e}")
    
    def create_default_config(self):
        """CrÃ©e un fichier de configuration par dÃ©faut"""
        
        # Copie le contenu YAML par dÃ©faut (celui d'au-dessus)
        # Pour cette dÃ©mo, on va crÃ©er une version simplifiÃ©e
        
        default_config = {
            'project': {
                'name': 'ThinkingIA',
                'version': '1.0.0',
                'random_seed': 42
            },
            'model': {
                'input_dim': 128,
                'hidden_dim': 512,
                'output_dim': 64,
                'device': 'auto'
            },
            'training': {
                'learning_rate': 0.0003,
                'batch_size': 32,
                'max_epochs': 100
            }
        }
        
        with open(self.config_path, 'w', encoding='utf-8') as f:
            yaml.dump(default_config, f, indent=2, default_flow_style=False)
        
        print(f"ğŸ“„ Fichier de configuration par dÃ©faut crÃ©Ã©: {self.config_path}")
        
        # Recharger
        with open(self.config_path, 'r', encoding='utf-8') as f:
            self.config_data = yaml.safe_load(f)
    
    def _post_process_config(self):
        """Post-traitement automatique de la configuration"""
        
        # Auto-dÃ©tection du device
        if self.config_data['model']['device'] == 'auto':
            self.config_data['model']['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # CrÃ©ation automatique des rÃ©pertoires
        if 'experiment' in self.config_data and 'paths' in self.config_data['experiment']:
            paths = self.config_data['experiment']['paths']
            base_dir = self.config_path.parent
            
            for path_name, path_value in paths.items():
                full_path = base_dir / path_value
                full_path.mkdir(parents=True, exist_ok=True)
                # Mettre Ã  jour avec le chemin absolu
                paths[path_name] = str(full_path)
        
        print(f"ğŸ”§ Post-traitement terminÃ© - Device: {self.config_data['model']['device']}")
    
    def _validate_config(self):
        """Validation basique de la configuration"""
        
        try:
            # VÃ©rifications obligatoires
            assert self.config_data['model']['input_dim'] > 0, "input_dim doit Ãªtre > 0"
            assert self.config_data['model']['hidden_dim'] > 0, "hidden_dim doit Ãªtre > 0"
            assert 0 < self.config_data['training']['learning_rate'] < 1, "learning_rate doit Ãªtre entre 0 et 1"
            
            # VÃ©rification de cohÃ©rence
            if 'attention_heads' in self.config_data['model']:
                hidden_dim = self.config_data['model']['hidden_dim']
                attention_heads = self.config_data['model']['attention_heads']
                assert hidden_dim % attention_heads == 0, f"hidden_dim ({hidden_dim}) doit Ãªtre divisible par attention_heads ({attention_heads})"
            
            print("âœ… Validation de la configuration rÃ©ussie")
            
        except AssertionError as e:
            raise ValueError(f"Erreur de validation: {e}")
        except KeyError as e:
            raise ValueError(f"ParamÃ¨tre manquant dans la configuration: {e}")
    
    def get(self, key_path: str, default=None):
        """
        RÃ©cupÃ¨re une valeur par chemin (ex: 'model.hidden_dim')
        """
        keys = key_path.split('.')
        value = self.config_data
        
        try:
            for key in keys:
                value = value[key]
            return value
        except (KeyError, TypeError):
            if default is not None:
                return default
            raise KeyError(f"ClÃ© '{key_path}' non trouvÃ©e dans la configuration")
    
    def __getitem__(self, key):
        """Permet l'accÃ¨s direct config['model']"""
        return self.config_data[key]
    
    def __getattr__(self, name):
        """Permet l'accÃ¨s par attribut config.model"""
        if name in self.config_data:
            return DictToObject(self.config_data[name])
        raise AttributeError(f"Configuration '{name}' non trouvÃ©e")

class DictToObject:
    """Convertit un dictionnaire en objet avec accÃ¨s par attribut"""
    def __init__(self, dictionary):
        for key, value in dictionary.items():
            if isinstance(value, dict):
                setattr(self, key, DictToObject(value))
            else:
                setattr(self, key, value)
    
    def __repr__(self):
        return f"Config({self.__dict__})"

# Instance globale de configuration
config = ConfigLoader()

# Pour compatibilitÃ© avec votre code existant
def get_config():
    return config
