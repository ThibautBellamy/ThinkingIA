"""
R√âSEAU DE NEURONES FROM SCRATCH
===============================

Ce script impl√©mente un r√©seau de neurones simple √† partir de z√©ro 
pour vous aider √† comprendre les m√©canismes fondamentaux de l'IA.

Auteur: Script d'apprentissage IA
Date: 2025
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List
import warnings
warnings.filterwarnings('ignore')

class ReseauNeurones:
    """
    Classe principale pour cr√©er et entra√Æner un r√©seau de neurones simple

    Architecture support√©e:
    - Couche d'entr√©e (nombre d'inputs variable)
    - Une couche cach√©e (taille configurable)
    - Couche de sortie (1 neurone pour classification binaire)
    """

    def __init__(self, nb_entrees: int, nb_neurones_caches: int, taux_apprentissage: float = 0.1):
        """
        Initialise le r√©seau de neurones

        Args:
            nb_entrees: Nombre d'entr√©es du r√©seau
            nb_neurones_caches: Nombre de neurones dans la couche cach√©e
            taux_apprentissage: Vitesse d'apprentissage (learning rate)
        """
        self.nb_entrees = nb_entrees
        self.nb_neurones_caches = nb_neurones_caches
        self.taux_apprentissage = taux_apprentissage

        # Initialisation des poids de mani√®re al√©atoire (valeurs petites)
        # Poids entre couche d'entr√©e et couche cach√©e
        self.poids_entree_cache = np.random.uniform(-1, 1, (self.nb_entrees, self.nb_neurones_caches))

        # Poids entre couche cach√©e et couche de sortie  
        self.poids_cache_sortie = np.random.uniform(-1, 1, (self.nb_neurones_caches, 1))

        # Biais pour chaque couche
        self.biais_cache = np.zeros((1, self.nb_neurones_caches))
        self.biais_sortie = np.zeros((1, 1))

        # Variables pour stocker les valeurs pendant la propagation
        self.entrees = None
        self.sortie_cache = None
        self.sortie_finale = None

        # Historique pour le graphique
        self.historique_erreur = []

    def fonction_sigmoid(self, x: np.ndarray) -> np.ndarray:
        """
        Fonction d'activation sigmo√Øde

        Formule: f(x) = 1 / (1 + e^(-x))
        Sortie: valeurs entre 0 et 1
        """
        # Clip pour √©viter les overflow
        x = np.clip(x, -250, 250)
        return 1 / (1 + np.exp(-x))

    def derivee_sigmoid(self, x: np.ndarray) -> np.ndarray:
        """
        D√©riv√©e de la fonction sigmo√Øde

        Formule: f'(x) = f(x) * (1 - f(x))
        N√©cessaire pour la r√©tropropagation
        """
        return x * (1 - x)

    def fonction_relu(self, x: np.ndarray) -> np.ndarray:
        """
        Fonction d'activation ReLU (Rectified Linear Unit)

        Formule: f(x) = max(0, x)
        Plus simple et souvent plus efficace que sigmoid
        """
        return np.maximum(0, x)

    def derivee_relu(self, x: np.ndarray) -> np.ndarray:
        """
        D√©riv√©e de la fonction ReLU

        Formule: f'(x) = 1 si x > 0, sinon 0
        """
        return (x > 0).astype(float)

    def propagation_avant(self, entrees: np.ndarray) -> np.ndarray:
        """
        PROPAGATION AVANT (Forward Pass)

        Calcule la sortie du r√©seau √©tape par √©tape:
        1. Entr√©es ‚Üí Couche cach√©e
        2. Couche cach√©e ‚Üí Sortie finale

        Args:
            entrees: Donn√©es d'entr√©e (shape: [nb_exemples, nb_entrees])

        Returns:
            Sortie du r√©seau (shape: [nb_exemples, 1])
        """
        self.entrees = entrees
        # Couche cach√©e avec ReLU
        z_cache = np.dot(entrees, self.poids_entree_cache) + self.biais_cache
        self.sortie_cache = self.fonction_relu(z_cache)
        # Couche de sortie avec sigmoid
        z_sortie = np.dot(self.sortie_cache, self.poids_cache_sortie) + self.biais_sortie
        self.sortie_finale = self.fonction_sigmoid(z_sortie)

        return self.sortie_finale

    def retropropagation(self, entrees: np.ndarray, sorties_attendues: np.ndarray):
        """
        R√âTROPROPAGATION (Backward Pass)

        Calcule les erreurs et met √† jour les poids:
        1. Calcule l'erreur en sortie
        2. Propage l'erreur vers les couches pr√©c√©dentes
        3. Met √† jour tous les poids et biais

        Args:
            entrees: Donn√©es d'entr√©e
            sorties_attendues: R√©sultats attendus
        """
        nb_exemples = entrees.shape[0]

        # √âTAPE 1: Calcul de l'erreur en sortie
        erreur_sortie = sorties_attendues - self.sortie_finale

        # Gradient pour la couche de sortie
        delta_sortie = erreur_sortie * self.derivee_sigmoid(self.sortie_finale)

        # √âTAPE 2: R√©tropropagation vers la couche cach√©e
        erreur_cache = delta_sortie.dot(self.poids_cache_sortie.T)
        # D√©riv√©e de ReLU pour la couche cach√©e
        delta_cache = erreur_cache * self.derivee_relu(self.sortie_cache)

        # √âTAPE 3: Mise √† jour des poids et biais

        # Mise √† jour poids couche cach√©e ‚Üí sortie
        self.poids_cache_sortie += self.sortie_cache.T.dot(delta_sortie) * self.taux_apprentissage / nb_exemples
        self.biais_sortie += np.sum(delta_sortie, axis=0, keepdims=True) * self.taux_apprentissage / nb_exemples

        # Mise √† jour poids entr√©e ‚Üí couche cach√©e  
        self.poids_entree_cache += entrees.T.dot(delta_cache) * self.taux_apprentissage / nb_exemples
        self.biais_cache += np.sum(delta_cache, axis=0, keepdims=True) * self.taux_apprentissage / nb_exemples

    def calculer_erreur(self, sorties_predites: np.ndarray, sorties_attendues: np.ndarray) -> float:
        """
        Calcule l'erreur moyenne quadratique (MSE)

        Formule: MSE = (1/n) * Œ£(y_r√©el - y_pr√©dit)¬≤
        """
        return np.mean((sorties_attendues - sorties_predites) ** 2)

    def entrainer(self, entrees: np.ndarray, sorties: np.ndarray, nb_epochs: int = 1000, 
                 afficher_progres: bool = True) -> List[float]:
        """
        ENTRA√éNEMENT DU R√âSEAU

        R√©p√®te le processus d'apprentissage sur plusieurs √©poques:
        1. Propagation avant
        2. Calcul de l'erreur
        3. R√©tropropagation
        4. Mise √† jour des poids

        Args:
            entrees: Donn√©es d'entra√Ænement
            sorties: R√©sultats attendus
            nb_epochs: Nombre d'it√©rations d'entra√Ænement
            afficher_progres: Afficher le progr√®s pendant l'entra√Ænement

        Returns:
            Historique des erreurs
        """
        self.historique_erreur = []

        for epoch in range(nb_epochs):
            # Propagation avant
            predictions = self.propagation_avant(entrees)

            # Calcul de l'erreur
            erreur = self.calculer_erreur(predictions, sorties)
            self.historique_erreur.append(erreur)

            # R√©tropropagation et mise √† jour des poids
            self.retropropagation(entrees, sorties)

            # Affichage du progr√®s
            if afficher_progres and epoch % (nb_epochs // 10) == 0:
                print(f"√âpoque {epoch}/{nb_epochs} - Erreur: {erreur:.6f}")

        if afficher_progres:
            print(f"Entra√Ænement termin√©! Erreur finale: {self.historique_erreur[-1]:.6f}")

        return self.historique_erreur

    def predire(self, entrees: np.ndarray) -> np.ndarray:
        """
        Fait des pr√©dictions sur de nouvelles donn√©es

        Args:
            entrees: Nouvelles donn√©es √† classifier

        Returns:
            Pr√©dictions (valeurs entre 0 et 1)
        """
        return self.propagation_avant(entrees)

    def classifier(self, entrees: np.ndarray, seuil: float = 0.5) -> np.ndarray:
        """
        Classifie les donn√©es en utilisant un seuil

        Args:
            entrees: Donn√©es √† classifier
            seuil: Seuil de d√©cision (par d√©faut 0.5)

        Returns:
            Classifications binaires (0 ou 1)
        """
        predictions = self.predire(entrees)
        return (predictions > seuil).astype(int)

    def afficher_architecture(self):
        """
        Affiche l'architecture du r√©seau
        """
        print("\n" + "="*50)
        print("ARCHITECTURE DU R√âSEAU DE NEURONES")
        print("="*50)
        print(f"Couche d'entr√©e:    {self.nb_entrees} neurones")
        print(f"Couche cach√©e:      {self.nb_neurones_caches} neurones (activation: sigmoid)")
        print(f"Couche de sortie:   1 neurone (activation: sigmoid)")
        print(f"Taux d'apprentissage: {self.taux_apprentissage}")
        print("="*50)

    def visualiser_apprentissage(self):
        """
        Affiche un graphique de l'√©volution de l'erreur pendant l'entra√Ænement
        """
        if not self.historique_erreur:
            print("Aucun entra√Ænement effectu√©. Lancez d'abord la m√©thode entrainer().")
            return

        plt.figure(figsize=(10, 6))
        plt.plot(self.historique_erreur, 'b-', linewidth=2, label='Erreur d\'entra√Ænement')
        plt.title('√âvolution de l\'erreur pendant l\'entra√Ænement', fontsize=14, fontweight='bold')
        plt.xlabel('√âpoque', fontsize=12)
        plt.ylabel('Erreur (MSE)', fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.yscale('log')  # √âchelle logarithmique pour mieux voir l'√©volution
        plt.tight_layout()
        plt.show()


# =============================================================================
# EXEMPLES D'UTILISATION
# =============================================================================

def exemple_xor():
    """
    Exemple classique: Apprendre la fonction XOR

    XOR n'est pas lin√©airement s√©parable, donc impossible 
    √† r√©soudre avec un perceptron simple. N√©cessite une couche cach√©e.
    """
    print("\n" + "="*60)
    print("EXEMPLE 1: FONCTION XOR")
    print("="*60)

    # Donn√©es XOR
    # Entr√©es: [A, B] ‚Üí Sortie: A XOR B
    entrees_xor = np.array([
        [0, 0],  # 0 XOR 0 = 0
        [0, 1],  # 0 XOR 1 = 1  
        [1, 0],  # 1 XOR 0 = 1
        [1, 1]   # 1 XOR 1 = 0
    ])

    sorties_xor = np.array([[0], [1], [1], [0]])

    print("Table de v√©rit√© XOR:")
    print("A | B | A XOR B")
    print("-" * 15)
    for i in range(len(entrees_xor)):
        a, b = entrees_xor[i]
        resultat = sorties_xor[i][0]
        print(f"{int(a)} | {int(b)} | {int(resultat)}")

    # Cr√©ation et entra√Ænement du r√©seau
    reseau = ReseauNeurones(nb_entrees=2, nb_neurones_caches=4, taux_apprentissage=1.0)
    reseau.afficher_architecture()

    print("\nEntra√Ænement en cours...")
    reseau.entrainer(entrees_xor, sorties_xor, nb_epochs=5000)

    # Test des pr√©dictions
    print("\nR√©sultats apr√®s entra√Ænement:")
    print("Entr√©e -> Pr√©diction (Classification)")
    print("-" * 35)

    for i in range(len(entrees_xor)):
        entree = entrees_xor[i:i+1]  # Une ligne √† la fois
        prediction = reseau.predire(entree)[0][0]
        classification = reseau.classifier(entree)[0][0]
        a, b = entrees_xor[i]
        print(f"[{int(a)}, {int(b)}] -> {prediction:.4f} ({int(classification)})")

    # Visualisation
    reseau.visualiser_apprentissage()

    return reseau

def exemple_et_ou():
    """
    Exemple simple: Apprendre les fonctions ET et OU

    Ces fonctions sont lin√©airement s√©parables, 
    plus faciles √† apprendre.
    """
    print("\n" + "="*60)
    print("EXEMPLE 2: FONCTIONS ET & OU")
    print("="*60)

    # Donn√©es pour fonction ET
    entrees = np.array([
        [0, 0],
        [0, 1], 
        [1, 0],
        [1, 1]
    ])

    sorties_et = np.array([[0], [0], [0], [1]])  # Fonction ET
    sorties_ou = np.array([[0], [1], [1], [1]])  # Fonction OU

    print("\nFonction ET:")
    reseau_et = ReseauNeurones(nb_entrees=2, nb_neurones_caches=2, taux_apprentissage=0.5)
    reseau_et.entrainer(entrees, sorties_et, nb_epochs=1000)

    print("R√©sultats fonction ET:")
    for i in range(len(entrees)):
        entree = entrees[i:i+1]
        pred = reseau_et.predire(entree)[0][0]
        classif = reseau_et.classifier(entree)[0][0]
        a, b = entrees[i]
        print(f"[{int(a)}, {int(b)}] ET -> {pred:.4f} ({int(classif)})")

    print("\nFonction OU:")
    reseau_ou = ReseauNeurones(nb_entrees=2, nb_neurones_caches=2, taux_apprentissage=0.5)
    reseau_ou.entrainer(entrees, sorties_ou, nb_epochs=1000)

    print("R√©sultats fonction OU:")
    for i in range(len(entrees)):
        entree = entrees[i:i+1]
        pred = reseau_ou.predire(entree)[0][0]
        classif = reseau_ou.classifier(entree)[0][0]
        a, b = entrees[i]
        print(f"[{int(a)}, {int(b)}] OU -> {pred:.4f} ({int(classif)})")
        
    # Visualisation
    reseau_et.visualiser_apprentissage()
    reseau_ou.visualiser_apprentissage()

def exemple_donnees_aleatoires():
    """
    Exemple avec des donn√©es g√©n√©r√©es al√©atoirement

    Montre comment le r√©seau peut apprendre des patterns 
    plus complexes.
    """
    print("\n" + "="*60)
    print("EXEMPLE 3: DONN√âES AL√âATOIRES")
    print("="*60)

    # G√©n√©ration de donn√©es al√©atoires
    np.random.seed(42)  # Pour reproductibilit√©
    nb_echantillons = 100

    # Donn√©es d'entr√©e: 2 features al√©atoires
    entrees = np.random.randn(nb_echantillons, 2)

    # Fonction cible: classification bas√©e sur une r√®gle simple
    # Si x1 + x2 > 0, alors classe 1, sinon classe 0
    sorties = ((entrees[:, 0] + entrees[:, 1]) > 0).astype(int).reshape(-1, 1)

    print(f"G√©n√©r√© {nb_echantillons} √©chantillons avec 2 features")
    print(f"R√®gle de classification: classe 1 si (x1 + x2) > 0")

    # Division en ensembles d'entra√Ænement et de test
    indices = np.random.permutation(nb_echantillons)
    split = int(0.8 * nb_echantillons)

    entrees_train = entrees[indices[:split]]
    sorties_train = sorties[indices[:split]]
    entrees_test = entrees[indices[split:]]
    sorties_test = sorties[indices[split:]]

    print(f"Entra√Ænement: {len(entrees_train)} √©chantillons")
    print(f"Test: {len(entrees_test)} √©chantillons")

    # Entra√Ænement
    reseau = ReseauNeurones(nb_entrees=2, nb_neurones_caches=5, taux_apprentissage=0.1)
    reseau.entrainer(entrees_train, sorties_train, nb_epochs=5000)

    # √âvaluation
    predictions_test = reseau.classifier(entrees_test)
    precision = np.mean(predictions_test == sorties_test)

    print(f"\nPr√©cision sur l'ensemble de test: {precision:.2%}")

    # Quelques exemples de pr√©dictions
    print("\nQuelques pr√©dictions:")
    print("Entr√©e [x1, x2] -> Pr√©diction (Attendu)")
    print("-" * 40)
    for i in range(min(5, len(entrees_test))):
        x1, x2 = entrees_test[i]
        pred = predictions_test[i][0]
        attendu = sorties_test[i][0]
        print(f"[{x1:.2f}, {x2:.2f}] -> {pred} ({attendu})")
    
    reseau.visualiser_apprentissage()

def guide_utilisation():
    """
    Guide d'utilisation du script
    """
    print("\n" + "="*80)
    print("GUIDE D'UTILISATION - R√âSEAU DE NEURONES FROM SCRATCH")
    print("="*80)

    print("""
Ce script vous permet de cr√©er et entra√Æner vos propres r√©seaux de neurones !

√âTAPES POUR CR√âER VOTRE R√âSEAU:

1. CR√âER LE R√âSEAU
   reseau = ReseauNeurones(nb_entrees=2, nb_neurones_caches=4, taux_apprentissage=0.1)

2. PR√âPARER VOS DONN√âES
   entrees = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])     # Donn√©es d'entr√©e
   sorties = np.array([[0], [1], [1], [0]])                  # R√©sultats attendus

3. ENTRA√éNER LE R√âSEAU
   reseau.entrainer(entrees, sorties, nb_epochs=1000)

4. FAIRE DES PR√âDICTIONS
   nouvelles_donnees = np.array([[0.5, 0.3]])
   prediction = reseau.predire(nouvelles_donnees)
   classification = reseau.classifier(nouvelles_donnees)

PARAM√àTRES IMPORTANTS:

‚Ä¢ nb_entrees: Nombre de caract√©ristiques de vos donn√©es
‚Ä¢ nb_neurones_caches: Complexit√© du r√©seau (plus = plus puissant mais plus lent)
‚Ä¢ taux_apprentissage: Vitesse d'apprentissage (0.01 √† 1.0)
‚Ä¢ nb_epochs: Nombre d'it√©rations d'entra√Ænement

CONSEILS:
‚Ä¢ Commencez avec des donn√©es simples (XOR, ET, OU)
‚Ä¢ Augmentez progressivement la complexit√©
‚Ä¢ Visualisez l'apprentissage avec reseau.visualiser_apprentissage()
‚Ä¢ Exp√©rimentez avec diff√©rents param√®tres

EXEMPLES DISPONIBLES:
‚Ä¢ exemple_xor() - Fonction XOR classique
‚Ä¢ exemple_et_ou() - Fonctions logiques simples  
‚Ä¢ exemple_donnees_aleatoires() - Donn√©es plus complexes
    """)

def main():
    """
    Fonction principale pour ex√©cuter les exemples
    """
    print("BIENVENUE DANS VOTRE PREMIER R√âSEAU DE NEURONES!")
    print("Ce script vous apprend √† cr√©er une IA from scratch \n")

    # Affichage du guide
    guide_utilisation()

    # Ex√©cution des exemples
    print("\n" + "="*80)
    print("EX√âCUTION DES EXEMPLES D'APPRENTISSAGE")
    print("="*80)

    # Exemple 1: XOR (le plus important)
    reseau_xor = exemple_xor()

    # Exemple 2: ET et OU
    exemple_et_ou()

    # Exemple 3: Donn√©es al√©atoires
    exemple_donnees_aleatoires()

    print("\n" + "="*80)
    print("F√âLICITATIONS! üéâ")
    print("="*80)
    print("Vous avez cr√©√© et entra√Æn√© vos premiers r√©seaux de neurones!")
    print("Vous pouvez maintenant:")
    print("‚Ä¢ Modifier les param√®tres pour exp√©rimenter")
    print("‚Ä¢ Cr√©er vos propres datasets")
    print("‚Ä¢ Tester d'autres fonctions")
    print("\nBonne exploration de l'IA! üöÄ")

if __name__ == "__main__":
    main()
