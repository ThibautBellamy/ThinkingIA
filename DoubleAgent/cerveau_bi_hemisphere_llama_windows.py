# cerveau_bi_hemisphere_llama_windows.py
import ollama
import torch
from transformers import CamembertForSequenceClassification, CamembertTokenizer
import time

class CerveauBiHemispheriqueLlama:
    """
    Architecture bi-h√©misph√©rique utilisant Llama 3.1 8B sur Windows
    """
    def __init__(self, 
                 model_questions_path="./finetuned/camembert-finetuned-questions",
                 model_concepts_path="./finetuned/camembert-finetuned-concepts"):
        
        print("üß† Initialisation du cerveau bi-h√©misph√©rique Windows + Llama 3.1...")
        
        # V√©rifier qu'Ollama fonctionne
        try:
            ollama.list()
            print("‚úÖ Connexion Ollama r√©ussie")
        except Exception as e:
            print(f"‚ùå Erreur Ollama: {e}")
            print("üí° Assurez-vous qu'Ollama est lanc√© : ollama serve")
            return
        
        # VOS MOD√àLES EXISTANTS (h√©misph√®res d'analyse)
        print("üìö Chargement de vos mod√®les fine-tun√©s...")
        try:
            self.questions_tokenizer = CamembertTokenizer.from_pretrained(model_questions_path)
            self.questions_model = CamembertForSequenceClassification.from_pretrained(model_questions_path)
            print("   ‚úÖ Mod√®le questions/affirmations charg√©")
        except Exception as e:
            print(f"   ‚ùå Erreur mod√®le questions: {e}")
            return
        
        try:
            self.concepts_tokenizer = CamembertTokenizer.from_pretrained(model_concepts_path)
            self.concepts_model = CamembertForSequenceClassification.from_pretrained(model_concepts_path)
            print("   ‚úÖ Mod√®le concepts charg√©")
        except Exception as e:
            print(f"   ‚ùå Erreur mod√®le concepts: {e}")
            return
        
        # Mapping des concepts
        self.concepts_labels = {
            0: "salutation_presentation", 1: "questionnement_interrogation", 
            2: "raisonnement_logique", 3: "affirmation_factuelle",
            4: "demande_action", 5: "expression_emotion", 6: "analyse_critique"
        }
        
        print("\n‚úÖ Cerveau bi-h√©misph√©rique initialis√© avec succ√®s !")
        print("   üéØ H√©misph√®re Logique : Analyse + Llama 3.1 (mode vulcain)")
        print("   üé® H√©misph√®re Cr√©atif : Analyse + Llama 3.1 (mode cr√©atif)")
        print("   üí¨ D√©bat interne : Confrontation des 2 approches")
    
    def analyser_avec_vos_modeles(self, enigme):
        """Utilise VOS mod√®les fine-tun√©s pour l'analyse pr√©liminaire"""
        # Classification type (question/affirmation)
        inputs_q = self.questions_tokenizer(enigme, return_tensors="pt", 
                                          padding=True, truncation=True, max_length=128)
        
        with torch.no_grad():
            outputs_q = self.questions_model(**inputs_q)
            probs_q = torch.nn.functional.softmax(outputs_q.logits, dim=-1)
            type_predicted = torch.argmax(probs_q, dim=1).item()
            type_confidence = torch.max(probs_q).item()
        
        type_phrase = "question" if type_predicted == 1 else "affirmation"
        
        # Classification conceptuelle
        inputs_c = self.concepts_tokenizer(enigme, return_tensors="pt",
                                         padding=True, truncation=True, max_length=128)
        
        with torch.no_grad():
            outputs_c = self.concepts_model(**inputs_c)
            probs_c = torch.nn.functional.softmax(outputs_c.logits, dim=-1)
            concept_predicted = torch.argmax(probs_c, dim=1).item()
            concept_confidence = torch.max(probs_c).item()
        
        concept_name = self.concepts_labels.get(concept_predicted, "inconnu")
        
        return {
            'type': type_phrase,
            'type_confidence': type_confidence,
            'concept': concept_name,
            'concept_confidence': concept_confidence
        }
    
    def interroger_llama(self, prompt, temperature=0.7):
        """Interroge Llama 3.1 8B via Ollama sur Windows"""
        try:
            response = ollama.generate(
                model='llama3.1',
                prompt=prompt,
                options={
                    'temperature': temperature,
                    'num_predict': 500,
                    'batch_size': 8,        # ‚úÖ AJOUT√â : Batch plus gros
                    'num_thread': 4,        # ‚úÖ AJOUT√â : Plus de threads
                    'top_k': 50,
                    'top_p': 0.9,
                }
            )
            return response['response']
        except Exception as e:
            return f"Erreur Llama: {str(e)}"
    
    def hemisphere_logique(self, enigme, analyse):
        """üß† H√âMISPH√àRE LOGIQUE : Mode Vulcain"""
        prompt_logique = f"""Tu es l'h√©misph√®re logique d'un cerveau artificiel. Tu raisonnes comme un Vulcain : logique pure, pr√©cision absolue, pas d'√©motion.

ANALYSE PR√âLIMINAIRE:
- Type: {analyse['type']} (confiance: {analyse['type_confidence']:.3f})
- Concept: {analyse['concept']} (confiance: {analyse['concept_confidence']:.3f})

√âNIGME √Ä R√âSOUDRE: {enigme}

CONSIGNES LOGIQUES:
1. Identifie les pr√©misses avec pr√©cision
2. Applique les r√®gles de logique formelle
3. D√©duis la conclusion par √©tapes rigoureuses
4. Critique ton propre raisonnement pour d√©tecter les failles
5. Formule une r√©ponse logique irr√©futable

RAISONNEMENT LOGIQUE VULCAIN:"""

        return self.interroger_llama(prompt_logique, temperature=0.3)
    
    def hemisphere_creatif(self, enigme, analyse):
        """üé® H√âMISPH√àRE CR√âATIF : Mode intuitif"""
        prompt_creatif = f"""Tu es l'h√©misph√®re cr√©atif d'un cerveau artificiel. Tu explores avec intuition, imagination et ouverture d'esprit.

ANALYSE PR√âLIMINAIRE:
- Type: {analyse['type']} (confiance: {analyse['type_confidence']:.3f})
- Concept: {analyse['concept']} (confiance: {analyse['concept_confidence']:.3f})

√âNIGME √Ä EXPLORER: {enigme}

CONSIGNES CR√âATIVES:
1. Explore des angles inattendus et originaux
2. Utilise ton intuition pour voir au-del√† de l'√©vidence
3. Propose des interpr√©tations multiples
4. Connecte des id√©es apparemment non-li√©es
5. Sois ouvert aux possibilit√©s paradoxales

APPROCHE CR√âATIVE INTUITIVE:"""

        return self.interroger_llama(prompt_creatif, temperature=0.9)
    
    def debat_interne(self, solution_logique, solution_creative, enigme):
        """üí¨ D√âBAT ENTRE LES DEUX H√âMISPH√àRES"""
        prompt_debat = f"""Tu es un mod√©rateur impartial qui organise un d√©bat entre deux h√©misph√®res c√©r√©braux.

√âNIGME ORIGINALE: {enigme}

SOLUTION H√âMISPH√àRE LOGIQUE:
{solution_logique}

SOLUTION H√âMISPH√àRE CR√âATIF:
{solution_creative}

TON R√îLE DE MOD√âRATEUR:
1. Compare les deux approches objectivement
2. Identifie les points forts et faibles de chaque solution
3. D√©termine laquelle est la plus pertinente ou si une synth√®se est possible
4. Explique ton raisonnement pour cette conclusion
5. Formule la r√©ponse finale optimale

SYNTH√àSE ET D√âCISION FINALE:"""

        return self.interroger_llama(prompt_debat, temperature=0.5)
    
    def resoudre_enigme_complete(self, enigme):
        """üß© PROCESSUS COMPLET Windows"""
        print(f"üß© R√©solution compl√®te de: '{enigme}'")
        print("=" * 80)
        
        # √âTAPE 1: Analyse avec vos mod√®les fine-tun√©s
        print("üìä Phase 1: Analyse pr√©liminaire avec vos mod√®les...")
        analyse = self.analyser_avec_vos_modeles(enigme)
        print(f"   üéØ Type d√©tect√©: {analyse['type']} (confiance: {analyse['type_confidence']:.3f})")
        print(f"   üè∑Ô∏è  Concept d√©tect√©: {analyse['concept']} (confiance: {analyse['concept_confidence']:.3f})")
        
        # √âTAPE 2: H√©misph√®re logique
        print("\nüß† Phase 2: Raisonnement h√©misph√®re logique avec Llama...")
        solution_logique = self.hemisphere_logique(enigme, analyse)
        print("   ‚úÖ Raisonnement logique g√©n√©r√©")
        
        # √âTAPE 3: H√©misph√®re cr√©atif  
        print("üé® Phase 3: Exploration h√©misph√®re cr√©atif avec Llama...")
        solution_creative = self.hemisphere_creatif(enigme, analyse)
        print("   ‚úÖ Approche cr√©ative g√©n√©r√©e")
        
        # √âTAPE 4: D√©bat et synth√®se
        print("üí¨ Phase 4: D√©bat interne et synth√®se finale...")
        synthese_finale = self.debat_interne(solution_logique, solution_creative, enigme)
        print("   ‚úÖ Synth√®se finale g√©n√©r√©e")
        
        # R√âSULTATS
        print("\n" + "=" * 80)
        print("üìã R√âSULTATS DU CERVEAU BI-H√âMISPH√âRIQUE")
        print("=" * 80)
        
        print(f"\nüß† H√âMISPH√àRE LOGIQUE:")
        print("-" * 40)
        print(solution_logique)
        
        print(f"\nüé® H√âMISPH√àRE CR√âATIF:")
        print("-" * 40)
        print(solution_creative)
        
        print(f"\nüí¨ SYNTH√àSE FINALE:")
        print("-" * 40)
        print(synthese_finale)
        
        return {
            'enigme': enigme,
            'analyse_preliminaire': analyse,
            'solution_logique': solution_logique,
            'solution_creative': solution_creative,
            'synthese_finale': synthese_finale
        }
    
    def mode_interactif_windows(self):
        """üéÆ Mode interactif Windows"""
        print("\nüéÆ MODE INTERACTIF WINDOWS - CERVEAU BI-H√âMISPH√âRIQUE")
        print("Soumettez vos √©nigmes au cerveau complet !")
        print("Tapez 'quitter' pour arr√™ter, 'test' pour une √©nigme de d√©monstration\n")
        
        while True:
            enigme = input("üß© Votre √©nigme: ").strip()
            
            if enigme.lower() in ['quitter', 'exit', 'stop']:
                print("üõë Cerveau bi-h√©misph√©rique en veille. √Ä bient√¥t !")
                break
            
            if enigme.lower() == 'test':
                enigme = "Si tous les chats sont des mammif√®res et F√©lix est un chat, que peut-on dire de F√©lix ?"
                print(f"üß™ √ânigme de test: {enigme}")
            
            if not enigme:
                print("‚ö†Ô∏è Veuillez saisir une √©nigme...")
                continue
            
            # R√©solution compl√®te
            start_time = time.time()
            self.resoudre_enigme_complete(enigme)
            end_time = time.time()
            
            print(f"\n‚è±Ô∏è  Temps de traitement: {end_time - start_time:.2f} secondes")
            print("\n" + "üîÑ" * 40 + "\n")

# Test du cerveau bi-h√©misph√©rique Windows
if __name__ == "__main__":
    print("ü™ü CERVEAU BI-H√âMISPH√âRIQUE WINDOWS + LLAMA 3.1")
    print("=" * 50)
    
    try:
        cerveau = CerveauBiHemispheriqueLlama()
        cerveau.mode_interactif_windows()
        
    except KeyboardInterrupt:
        print("\nüõë Arr√™t demand√© par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur g√©n√©rale: {e}")
        print("\nüí° V√©rifications √† faire:")
        print("   1. Ollama est install√© : ollama --version")
        print("   2. Llama 3.1 est t√©l√©charg√© : ollama list")
        print("   3. Vos mod√®les fine-tun√©s sont disponibles")
