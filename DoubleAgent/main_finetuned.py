# main_finetuned.py
from transformers import CamembertForSequenceClassification, CamembertTokenizer, AutoModel, AutoTokenizer
import torch
import numpy as np
import sys

class BaseModeleFrancaisFineTune:
    """
    Version am√©lior√©e qui utilise le mod√®le fine-tun√© pour questions/affirmations
    """
    def __init__(self, model_path="./camembert-finetuned-questions"):
        # Charger le mod√®le fine-tun√© pour classification
        print(f"üéØ Chargement du mod√®le fine-tun√© depuis {model_path}")
        
        try:
            self.finetuned_tokenizer = CamembertTokenizer.from_pretrained(model_path)
            self.finetuned_model = CamembertForSequenceClassification.from_pretrained(model_path)
            print("‚úÖ Mod√®le fine-tun√© charg√© avec succ√®s !")
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le fine-tun√©: {e}")
            print("üìù Utilisation du mod√®le de base √† la place...")
            model_path = "camembert-base"
        
        # Charger aussi le mod√®le de base pour les embeddings
        self.base_tokenizer = AutoTokenizer.from_pretrained("camembert-base")
        self.base_model = AutoModel.from_pretrained("camembert-base")
        
        print("üá´üá∑ Syst√®me hybride initialis√© !")
        print("   üéØ Classification questions/affirmations : Mod√®le fine-tun√©")
        print("   üîç Analyse s√©mantique g√©n√©rale : Mod√®le de base")
    
    def classifier_question_affirmation(self, phrase):
        """
        Classification pr√©cise avec le mod√®le fine-tun√©
        """
        inputs = self.finetuned_tokenizer(
            phrase,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=128
        )
        
        with torch.no_grad():
            outputs = self.finetuned_model(**inputs)
            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_class = torch.argmax(probabilities, dim=1).item()
            confidence = torch.max(probabilities).item()
        
        # 0 = affirmation, 1 = question
        type_phrase = "question" if predicted_class == 1 else "affirmation"
        
        return {
            'type': type_phrase,
            'confiance': confidence,
            'score_question': probabilities[0][1].item(),
            'score_affirmation': probabilities[0][0].item(),
            'classe_predite': predicted_class
        }
    
    def encoder_texte(self, texte):
        """
        Encodage s√©mantique avec le mod√®le de base (pour autres analyses)
        """
        inputs = self.base_tokenizer(
            texte, 
            return_tensors="pt", 
            padding=True, 
            truncation=True,
            max_length=512
        )
        
        with torch.no_grad():
            outputs = self.base_model(**inputs)
            embeddings = outputs.last_hidden_state.mean(dim=1)
        
        return embeddings.numpy()
    
    def similarite_semantique(self, texte1, texte2):
        """
        Calcule la similarit√© s√©mantique (version conserv√©e)
        """
        emb1 = self.encoder_texte(texte1)
        emb2 = self.encoder_texte(texte2)
        
        similarity = np.dot(emb1, emb2.T) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        return float(similarity[0][0])
    
    def analyser_phrase_complete(self, phrase):
        """
        Analyse compl√®te : classification fine-tun√©e + analyse s√©mantique
        """
        # Classification question/affirmation avec fine-tuning
        classification = self.classifier_question_affirmation(phrase)
        
        # Tokenisation
        tokens = self.base_tokenizer.tokenize(phrase)
        
        # Concepts plus orient√©s "raisonnement" pour votre projet
        concepts_references = {
            "raisonnement_logique": "raisonnement logique d√©duction inf√©rence",
            "questionnement": "question interrogation pourquoi comment",
            "affirmation_certitude": "affirmation certitude √©vident s√ªr",
            "analyse_critique": "analyser critiquer examiner √©valuer", 
            "r√©solution_probl√®me": "r√©soudre solution probl√®me m√©thode",
            "math√©matiques": "nombre calcul √©quation math√©matique",
            "philosophie": "existence pens√©e conscience √™tre",
            "action_concr√®te": "faire agir r√©aliser ex√©cuter",
        }
        
        scores_concepts = {}
        for concept, mots_cles in concepts_references.items():
            score = self.similarite_semantique(phrase, mots_cles)
            scores_concepts[concept] = score
        
        # Trier par score
        concepts_tries = sorted(scores_concepts.items(), key=lambda x: x[1], reverse=True)
        
        return {
            'phrase': phrase,
            'classification': classification,
            'tokens': tokens,
            'nombre_tokens': len(tokens),
            'concepts': concepts_tries[:3],  # Top 3 concepts
            'concept_principal': concepts_tries[0]
        }
    
    def test_comparaison_avant_apres(self):
        """
        Compare les r√©sultats avant/apr√®s fine-tuning
        """
        phrases_test = [
            "Qu'est-ce que tu penses de cette solution ?",
            "Comment r√©soudre ce probl√®me complexe ?", 
            "Es-tu capable de raisonner logiquement ?",
            "Je pense que cette approche est correcte.",
            "Cette m√©thode fonctionne tr√®s bien.",
            "Il est √©vident que 2+2 fait 4.",
            "Peux-tu m'expliquer le raisonnement ?",
            "La logique dicte cette conclusion."
        ]
        
        print("\nüß™ TEST COMPARATIF - Mod√®le fine-tun√© vs m√©thodes classiques")
        print("=" * 80)
        
        for phrase in phrases_test:
            print(f"\nüìù Phrase: '{phrase}'")
            
            # Analyse avec mod√®le fine-tun√©
            analyse_complete = self.analyser_phrase_complete(phrase)
            classif = analyse_complete['classification']
            
            print(f"üéØ Fine-tun√© : {classif['type']} (confiance: {classif['confiance']:.3f})")
            print(f"   Scores: Q={classif['score_question']:.3f} | A={classif['score_affirmation']:.3f}")
            
            # Analyse s√©mantique classique (pour comparaison)
            sim_question = self.similarite_semantique(phrase, "C'est une question")
            sim_affirmation = self.similarite_semantique(phrase, "C'est une affirmation")
            
            type_classique = "question" if sim_question > sim_affirmation else "affirmation"
            print(f"üîç Classique: {type_classique} (Q={sim_question:.3f} | A={sim_affirmation:.3f})")
            
            # Concept principal d√©tect√©
            concept_principal = analyse_complete['concept_principal']
            print(f"üè∑Ô∏è  Concept: {concept_principal[0]} ({concept_principal[1]:.3f})")
    
    def mode_chat_ameliore(self):
        """
        Mode chat avec classification fine-tun√©e
        """
        print("\nü§ñ Mode chat am√©lior√© - Powered by Fine-Tuning!")
        print("üí° Le mod√®le utilise maintenant la classification fine-tun√©e")
        print("Tapez 'quitter' pour sortir\n")
        
        while True:
            user_input = input("Vous: ").strip()
            
            if user_input.lower() in ['quitter', 'exit']:
                print("Chat termin√© !")
                break
            
            # Analyse compl√®te
            analyse = self.analyser_phrase_complete(user_input)
            classif = analyse['classification']
            
            print(f"üéØ Type d√©tect√©: {classif['type']} (confiance: {classif['confiance']:.3f})")
            
            # R√©ponse adapt√©e au type
            if classif['type'] == 'question':
                if classif['confiance'] > 0.8:
                    print("Mod√®le: Je d√©tecte une question claire. Laissez-moi analyser...")
                else:
                    print("Mod√®le: Cela semble √™tre une question. Pr√©cisez si besoin.")
            else:  # affirmation
                if classif['confiance'] > 0.8:
                    print("Mod√®le: J'enregistre cette affirmation. Int√©ressant point de vue.")
                else:
                    print("Mod√®le: Je note cette information. Continuez...")
            
            # Concept principal
            concept = analyse['concept_principal']
            if concept[1] > 0.4:  # Seuil de pertinence
                print(f"üí≠ Concept d√©tect√©: {concept[0]} (score: {concept[1]:.3f})")

def main():
    print("ü§ñ IA de Raisonnement Autonome - Version Fine-Tun√©e")
    
    if len(sys.argv) > 1:
        mode = sys.argv[1].lower()
    else:
        mode = input("Mode (test/comparaison/chat): ").strip().lower()
    
    # Initialisation du mod√®le fine-tun√©
    try:
        modele = BaseModeleFrancaisFineTune()
    except Exception as e:
        print(f"‚ùå Erreur d'initialisation: {e}")
        sys.exit(1)
    
    if mode == "test":
        phrases_test = [
            "Qu'est-ce que tu penses ?",
            "Comment √ßa marche ?", 
            "Je pense que c'est bien.",
            "C'est vraiment int√©ressant.",
            "Es-tu d'accord ?",
            "Il faut que tu comprennes."
        ]
        
        print("\nüß™ Test rapide du mod√®le fine-tun√©:")
        for phrase in phrases_test:
            resultat = modele.classifier_question_affirmation(phrase)
            print(f"'{phrase}' ‚Üí {resultat['type']} ({resultat['confiance']:.3f})")
            
    elif mode == "comparaison":
        modele.test_comparaison_avant_apres()
        
    elif mode == "chat":
        modele.mode_chat_ameliore()
        
    else:
        print("Mode non reconnu. Utilisez 'test', 'comparaison' ou 'chat'")

if __name__ == "__main__":
    main()
