# cerveau_bi_hemisphere_parallele.py
import ollama
import torch
from transformers import CamembertForSequenceClassification, CamembertTokenizer
import concurrent.futures
import threading
import time

class CerveauBiHemispheriqueParallele:
    """
    Architecture bi-h√©misph√©rique COMPL√àTE avec traitement parall√®le optimis√©
    Utilise VOS mod√®les fine-tun√©s + Llama 3.1 8B en parall√®le
    """
    def __init__(self, 
                 model_questions_path="./camembert-finetuned-questions",
                 model_concepts_path="./camembert-finetuned-concepts"):
        
        print("üß† Initialisation du cerveau bi-h√©misph√©rique PARALL√àLE...")
        
        # Lock pour √©viter les conflits GPU avec les mod√®les CamemBERT
        self.gpu_lock = threading.Lock()
        
        # V√©rifier qu'Ollama fonctionne
        try:
            ollama.list()
            print("‚úÖ Connexion Ollama r√©ussie")
        except Exception as e:
            print(f"‚ùå Erreur Ollama: {e}")
            print("üí° Assurez-vous qu'Ollama est lanc√©")
            return
        
        # VOS MOD√àLES FINE-TUN√âS (chargement s√©curis√©)
        print("üìö Chargement de vos mod√®les fine-tun√©s...")
        try:
            with self.gpu_lock:
                self.questions_tokenizer = CamembertTokenizer.from_pretrained(model_questions_path)
                self.questions_model = CamembertForSequenceClassification.from_pretrained(model_questions_path)
                self.questions_model.eval()  # Mode √©valuation pour performance
                print("   ‚úÖ Mod√®le questions/affirmations charg√©")
                
                self.concepts_tokenizer = CamembertTokenizer.from_pretrained(model_concepts_path)
                self.concepts_model = CamembertForSequenceClassification.from_pretrained(model_concepts_path)
                self.concepts_model.eval()  # Mode √©valuation pour performance
                print("   ‚úÖ Mod√®le concepts charg√©")
        except Exception as e:
            print(f"   ‚ùå Erreur chargement mod√®les: {e}")
            return
        
        # Mapping des concepts
        self.concepts_labels = {
            0: "salutation_presentation", 1: "questionnement_interrogation", 
            2: "raisonnement_logique", 3: "affirmation_factuelle",
            4: "demande_action", 5: "expression_emotion", 6: "analyse_critique"
        }
        
        print("\n‚úÖ Cerveau bi-h√©misph√©rique PARALL√àLE initialis√© !")
        print("   üéØ H√©misph√®re Logique : Vos mod√®les + Llama 3.1 (mode vulcain)")
        print("   üé® H√©misph√®re Cr√©atif : Vos mod√®les + Llama 3.1 (mode cr√©atif)")
        print("   ‚ö° Traitement : PARALL√àLE optimis√© GPU + CPU")
        print("   üí¨ D√©bat interne : Confrontation des 2 approches")
    
    def analyser_avec_vos_modeles_thread_safe(self, enigme):
        """
        Analyse thread-safe avec VOS mod√®les fine-tun√©s
        Utilise un lock pour √©viter les conflits GPU
        """
        with self.gpu_lock:  # Protection contre acc√®s concurrent GPU
            # Classification type (question/affirmation)
            inputs_q = self.questions_tokenizer(
                enigme, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=128
            )
            
            with torch.no_grad():
                outputs_q = self.questions_model(**inputs_q)
                probs_q = torch.nn.functional.softmax(outputs_q.logits, dim=-1)
                type_predicted = torch.argmax(probs_q, dim=1).item()
                type_confidence = torch.max(probs_q).item()
            
            type_phrase = "question" if type_predicted == 1 else "affirmation"
            
            # Classification conceptuelle
            inputs_c = self.concepts_tokenizer(
                enigme, 
                return_tensors="pt",
                padding=True, 
                truncation=True, 
                max_length=128
            )
            
            with torch.no_grad():
                outputs_c = self.concepts_model(**inputs_c)
                probs_c = torch.nn.functional.softmax(outputs_c.logits, dim=-1)
                concept_predicted = torch.argmax(probs_c, dim=1).item()
                concept_confidence = torch.max(probs_c).item()
            
            concept_name = self.concepts_labels.get(concept_predicted, "inconnu")
            
            return {
                'type': type_phrase,
                'type_confidence': type_confidence,
                'concept': concept_name,
                'concept_confidence': concept_confidence
            }
    
    def interroger_llama_optimise(self, prompt, temperature=0.7, thread_id=""):
        """
        Version optimis√©e pour interroger Llama avec identification thread
        """
        try:
            response = ollama.generate(
                model='llama3.1',
                prompt=prompt,
                options={
                    'temperature': temperature,
                    'num_predict': 500,
                    'batch_size': 4,      # Optimisation batch
                    'num_thread': 2,      # Plus de threads CPU
                    'top_k': 50,
                    'top_p': 0.9,
                }
            )
            print(f"   ‚úÖ Thread {thread_id} termin√©")
            return response['response']
        except Exception as e:
            print(f"   ‚ùå Erreur Thread {thread_id}: {str(e)}")
            return f"Erreur Llama Thread {thread_id}: {str(e)}"
    
    def hemisphere_logique_thread(self, enigme, analyse):
        """
        üß† H√âMISPH√àRE LOGIQUE - Version thread optimis√©e
        """
        thread_id = "LOGIQUE"
        print(f"   üß† D√©marrage h√©misph√®re {thread_id}...")
        
        prompt_logique = f"""Tu es l'h√©misph√®re logique d'un cerveau artificiel. Tu raisonnes comme un Vulcain : logique pure, pr√©cision absolue.

ANALYSE PR√âLIMINAIRE par mod√®les sp√©cialis√©s:
- Type d√©tect√©: {analyse['type']} (confiance: {analyse['type_confidence']:.3f})
- Concept d√©tect√©: {analyse['concept']} (confiance: {analyse['concept_confidence']:.3f})

√âNIGME √Ä R√âSOUDRE LOGIQUEMENT: {enigme}

CONSIGNES LOGIQUES STRICTES:
1. Identifie les pr√©misses avec pr√©cision math√©matique
2. Applique les r√®gles de logique formelle (syllogisme, modus ponens, etc.)
3. D√©duis la conclusion par √©tapes rigoureuses et v√©rifiables
4. Critique ton propre raisonnement pour d√©tecter les failles logiques
5. Formule une r√©ponse irr√©futable bas√©e sur la logique pure

RAISONNEMENT LOGIQUE VULCAIN:"""

        result = self.interroger_llama_optimise(prompt_logique, temperature=0.2, thread_id=thread_id)
        return result
    
    def hemisphere_creatif_thread(self, enigme, analyse):
        """
        üé® H√âMISPH√àRE CR√âATIF - Version thread optimis√©e
        """
        thread_id = "CR√âATIF"
        print(f"   üé® D√©marrage h√©misph√®re {thread_id}...")
        
        prompt_creatif = f"""Tu es l'h√©misph√®re cr√©atif d'un cerveau artificiel. Tu explores avec intuition, imagination et ouverture d'esprit.

ANALYSE PR√âLIMINAIRE par mod√®les sp√©cialis√©s:
- Type d√©tect√©: {analyse['type']} (confiance: {analyse['type_confidence']:.3f})
- Concept d√©tect√©: {analyse['concept']} (confiance: {analyse['concept_confidence']:.3f})

√âNIGME √Ä EXPLORER CR√âATIVEMENT: {enigme}

CONSIGNES CR√âATIVES LIBRES:
1. Explore des angles inattendus et des perspectives originales
2. Utilise ton intuition pour voir au-del√† de l'√©vidence logique
3. Propose des interpr√©tations multiples et des solutions alternatives
4. Connecte des id√©es apparemment non-li√©es de mani√®re innovante
5. Sois ouvert aux possibilit√©s paradoxales et aux solutions non-conventionnelles
6. Offre une perspective humaine et √©motionnelle √† l'√©nigme

APPROCHE CR√âATIVE INTUITIVE:"""

        result = self.interroger_llama_optimise(prompt_creatif, temperature=0.9, thread_id=thread_id)
        return result
    
    def debat_interne_optimise(self, solution_logique, solution_creative, enigme, analyse):
        """
        üí¨ D√âBAT OPTIMIS√â entre les deux h√©misph√®res
        """
        print("   üí¨ D√©marrage du d√©bat interne...")
        
        prompt_debat = f"""Tu es un mod√©rateur expert qui organise un d√©bat constructif entre deux h√©misph√®res c√©r√©braux.

√âNIGME ORIGINALE: {enigme}
ANALYSE PR√âLIMINAIRE: Type={analyse['type']}, Concept={analyse['concept']}

SOLUTION H√âMISPH√àRE LOGIQUE (Vulcain):
{solution_logique}

SOLUTION H√âMISPH√àRE CR√âATIF (Intuitif):
{solution_creative}

TON R√îLE DE MOD√âRATEUR EXPERT:
1. Compare objectivement les deux approches en analysant leurs m√©rites respectifs
2. Identifie les points forts et les limites de chaque solution
3. √âvalue la pertinence de chaque approche selon le type d'√©nigme
4. D√©termine si une approche est sup√©rieure ou si une synth√®se est optimale
5. Explique ton raisonnement de mani√®re claire et structur√©e
6. Formule la r√©ponse finale la plus compl√®te et pr√©cise possible

SYNTH√àSE FINALE DU D√âBAT ET D√âCISION:"""

        result = self.interroger_llama_optimise(prompt_debat, temperature=0.5, thread_id="D√âBAT")
        return result
    
    def resoudre_enigme_parallele_complete(self, enigme):
        """
        üß© PROCESSUS COMPLET PARALL√âLIS√â avec vos mod√®les fine-tun√©s
        """
        start_time = time.time()
        print(f"üß© R√©solution PARALL√àLE de: '{enigme}'")
        print("=" * 80)
        
        # √âTAPE 1: Analyse pr√©liminaire avec VOS mod√®les fine-tun√©s
        print("üìä Phase 1: Analyse pr√©liminaire avec vos mod√®les CamemBERT...")
        analyse_start = time.time()
        
        analyse = self.analyser_avec_vos_modeles_thread_safe(enigme)
        
        analyse_time = time.time() - analyse_start
        print(f"   üéØ Type d√©tect√©: {analyse['type']} (confiance: {analyse['type_confidence']:.3f})")
        print(f"   üè∑Ô∏è  Concept d√©tect√©: {analyse['concept']} (confiance: {analyse['concept_confidence']:.3f})")
        print(f"   ‚è±Ô∏è  Temps analyse: {analyse_time:.2f}s")
        
        # √âTAPE 2: TRAITEMENT PARALL√àLE des deux h√©misph√®res
        print("\nüöÄ Phase 2: Lancement PARALL√àLE des h√©misph√®res avec Llama...")
        parallel_start = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            # Soumettre les 2 t√¢ches en parall√®le
            future_logique = executor.submit(self.hemisphere_logique_thread, enigme, analyse)
            future_creatif = executor.submit(self.hemisphere_creatif_thread, enigme, analyse)
            
            # Attendre les r√©sultats
            solution_logique = future_logique.result()
            solution_creative = future_creatif.result()
        
        parallel_time = time.time() - parallel_start
        print(f"   ‚úÖ Les 2 h√©misph√®res termin√©s en parall√®le !")
        print(f"   ‚è±Ô∏è  Temps parall√®le: {parallel_time:.2f}s")
        
        # √âTAPE 3: D√©bat et synth√®se finale
        print("\nüí¨ Phase 3: D√©bat interne et synth√®se finale...")
        debat_start = time.time()
        
        synthese_finale = self.debat_interne_optimise(solution_logique, solution_creative, enigme, analyse)
        
        debat_time = time.time() - debat_start
        print(f"   ‚úÖ Synth√®se finale g√©n√©r√©e")
        print(f"   ‚è±Ô∏è  Temps d√©bat: {debat_time:.2f}s")
        
        # AFFICHAGE DES R√âSULTATS COMPLETS
        total_time = time.time() - start_time
        print("\n" + "=" * 80)
        print("üìã R√âSULTATS DU CERVEAU BI-H√âMISPH√âRIQUE PARALL√àLE")
        print("=" * 80)
        
        print(f"\nüìä ANALYSE PR√âLIMINAIRE (Vos mod√®les fine-tun√©s):")
        print("-" * 50)
        print(f"Type: {analyse['type']} (confiance: {analyse['type_confidence']:.3f})")
        print(f"Concept: {analyse['concept']} (confiance: {analyse['concept_confidence']:.3f})")
        
        print(f"\nüß† H√âMISPH√àRE LOGIQUE (Llama 3.1 mode Vulcain):")
        print("-" * 50)
        print(solution_logique)
        
        print(f"\nüé® H√âMISPH√àRE CR√âATIF (Llama 3.1 mode intuitif):")
        print("-" * 50)
        print(solution_creative)
        
        print(f"\nüí¨ SYNTH√àSE FINALE (D√©bat mod√©r√©):")
        print("-" * 50)
        print(synthese_finale)
        
        print(f"\n‚è±Ô∏è  PERFORMANCES:")
        print(f"   Analyse: {analyse_time:.2f}s | Parall√®le: {parallel_time:.2f}s | D√©bat: {debat_time:.2f}s")
        print(f"   TOTAL: {total_time:.2f}s")
        
        return {
            'enigme': enigme,
            'analyse_preliminaire': analyse,
            'solution_logique': solution_logique,
            'solution_creative': solution_creative,
            'synthese_finale': synthese_finale,
            'temps_total': total_time,
            'temps_analyse': analyse_time,
            'temps_parallele': parallel_time,
            'temps_debat': debat_time
        }
    
    def mode_test_performance(self):
        """
        üß™ Mode de test des performances parall√®les
        """
        enigmes_test = [
            "Si tous les chats sont des mammif√®res et F√©lix est un chat, que peut-on dire de F√©lix ?",
            "Alice est plus √¢g√©e que Bob. Bob est plus √¢g√© que Charlie. Qui est le plus √¢g√© ?",
            "Comment r√©soudre l'√©quation : 2x + 5 = 13 ?",
            "Un barbier rase tous les hommes qui ne se rasent pas eux-m√™mes. Qui rase le barbier ?"
        ]
        
        print("üß™ TEST DE PERFORMANCE - Architecture bi-h√©misph√©rique parall√®le")
        print("=" * 80)
        
        temps_total = 0
        for i, enigme in enumerate(enigmes_test, 1):
            print(f"\nüî¢ Test {i}/{len(enigmes_test)}")
            print("-" * 60)
            
            resultat = self.resoudre_enigme_parallele_complete(enigme)
            temps_total += resultat['temps_total']
            
            print("\n" + "üîÑ" * 40 + "\n")
        
        print(f"üìä BILAN PERFORMANCE GLOBALE:")
        print(f"   Temps total: {temps_total:.2f}s")
        print(f"   Temps moyen par √©nigme: {temps_total/len(enigmes_test):.2f}s")
        print(f"   √ânigmes trait√©es: {len(enigmes_test)}")
    
    def mode_interactif_optimise(self):
        """
        üéÆ Mode interactif avec architecture parall√®le optimis√©e
        """
        print("\nüéÆ MODE INTERACTIF - CERVEAU BI-H√âMISPH√âRIQUE PARALL√àLE")
        print("Architecture compl√®te : Vos mod√®les fine-tun√©s + Llama 3.1 en parall√®le")
        print("Commandes : 'test' = √©nigme d√©mo | 'perf' = test performance | 'quitter' = sortir\n")
        
        while True:
            enigme = input("üß© Votre √©nigme: ").strip()
            
            if enigme.lower() in ['quitter', 'exit', 'stop']:
                print("üõë Cerveau bi-h√©misph√©rique en veille. √Ä bient√¥t !")
                break
            
            if enigme.lower() == 'test':
                enigme = "Si tous les chats sont des mammif√®res et F√©lix est un chat, que peut-on dire de F√©lix ?"
                print(f"üß™ √ânigme de test: {enigme}")
            
            if enigme.lower() == 'perf':
                self.mode_test_performance()
                continue
            
            if not enigme:
                print("‚ö†Ô∏è Veuillez saisir une √©nigme...")
                continue
            
            # R√©solution compl√®te parall√®le
            self.resoudre_enigme_parallele_complete(enigme)
            print("\n" + "üîÑ" * 40 + "\n")

# LANCEMENT DU CERVEAU BI-H√âMISPH√âRIQUE PARALL√àLE
if __name__ == "__main__":
    print("ü™ü CERVEAU BI-H√âMISPH√âRIQUE PARALL√àLE - Windows + Llama 3.1")
    print("Vos mod√®les CamemBERT fine-tun√©s + Traitement parall√®le optimis√©")
    print("=" * 70)
    
    try:
        # Variables d'environnement pour optimiser Ollama (optionnel)
        import os
        os.environ['OLLAMA_NUM_PARALLEL'] = '4'
        os.environ['OLLAMA_FLASH_ATTENTION'] = '1'
        
        # Initialisation du cerveau
        cerveau = CerveauBiHemispheriqueParallele()
        
        # Mode interactif optimis√©
        cerveau.mode_interactif_optimise()
        
    except KeyboardInterrupt:
        print("\nüõë Arr√™t demand√© par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur g√©n√©rale: {e}")
        print("\nüí° V√©rifications √† faire:")
        print("   1. Ollama fonctionne : ollama --version")
        print("   2. Llama 3.1 disponible : ollama list")
        print("   3. Vos mod√®les fine-tun√©s sont accessibles")
        print("   4. GPU NVIDIA drivers √† jour")
